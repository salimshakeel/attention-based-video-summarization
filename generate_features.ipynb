{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39140f8-3243-4d4c-97c2-221dc4a67ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10692f81-9dda-4912-ac50-4a4bee7af0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_summary import generate_summary\n",
    "from evaluation_metrics import evaluate_summary\n",
    "from layers.summarizer import PGL_SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41cc93ca-00d0-486c-9fad-149906d8dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "video_folder = './Videos/'  # Folder containing your input videos\n",
    "h5_file_path = os.path.join(\"h5file_folder\", \"my_data.h5\")  # Output .h5 files\n",
    "dataset_prefix = 'custom_dataset_01'\n",
    "fps = 15  # Target frame sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67101d5c-28ce-44e4-83f3-6171c175969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(h5_file_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "212006ff-0b47-420d-8090-783b086d2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da37971-98a7-4dc3-aaa1-6230883d78e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mobility\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mobility\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load pretrained GoogLeNet\n",
    "googlenet = models.googlenet(pretrained=True)\n",
    "googlenet = googlenet.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4c30df8-9aca-4a24-9bf5-0eadb315351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Modify it to return features from 'avgpool' (1024-dim)\n",
    "feature_extractor = torch.nn.Sequential(\n",
    "    googlenet.conv1,\n",
    "    googlenet.maxpool1,\n",
    "    googlenet.conv2,\n",
    "    googlenet.conv3,\n",
    "    googlenet.maxpool2,\n",
    "    googlenet.inception3a,\n",
    "    googlenet.inception3b,\n",
    "    googlenet.maxpool3,\n",
    "    googlenet.inception4a,\n",
    "    googlenet.inception4b,\n",
    "    googlenet.inception4c,\n",
    "    googlenet.inception4d,\n",
    "    googlenet.inception4e,\n",
    "    googlenet.maxpool4,\n",
    "    googlenet.inception5a,\n",
    "    googlenet.inception5b,\n",
    "    googlenet.avgpool,  # shape: [1, 1024, 1, 1]\n",
    "    torch.nn.Flatten(),  # shape: [1, 1024]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf1a046a-7451-4285-b463-b5cf5733deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Image pre-processing transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "        std=[0.229, 0.224, 0.225]    # ImageNet stds\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "088d68a4-06b3-4d29-9574-68f24714acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1a2f770-4513-4ed6-b90c-ab8c278b86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract features from video\n",
    "def extract_video_features(video_path, frame_rate=15):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frames = []\n",
    "    picks = []\n",
    "    count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if int(count % round(fps // frame_rate)) == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame_rgb)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feature = feature_extractor(input_tensor)  # shape: [1, 1024]\n",
    "                frames.append(feature.squeeze(0).cpu().numpy())  # shape: [1024]\n",
    "                picks.append(count)\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames, picks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "448e54f3-7cc6-4ee8-a74c-735c7f7dcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"./Videos/1585497-hd_1920_1080_30fps.mp4\"  # replace with your actual video path\n",
    "frames, picks = extract_video_features(video_path)\n",
    "frames_np = np.stack(frames)  # shape: [T, 1024]\n",
    "frames_tensor = torch.tensor(frames_np, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3755fdfc-1b84-4014-8721-ea473a725b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PGL_SUM(input_size=1024, output_size=1024, num_segments=4, heads=8,\n",
    "                                fusion=\"add\", pos_enc=\"absolute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e23d932c-bf5c-4c70-93d7-1d82b8ac554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_change_points(picks, n_frames, segments=5):\n",
    "    seg_len = len(picks) // segments\n",
    "    change_points = []\n",
    "    frame_per_seg = []\n",
    "    for i in range(segments):\n",
    "        start = i * seg_len\n",
    "        end = (i + 1) * seg_len - 1 if i < segments - 1 else len(picks) - 1\n",
    "        change_points.append([picks[start], picks[end]])\n",
    "        frame_per_seg.append(picks[end] - picks[start] + 1)\n",
    "    return np.array(change_points), np.array(frame_per_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ef78b9a-f092-4f63-8be1-0010cbabf1bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_method' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m     scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     50\u001b[0m     summary \u001b[38;5;241m=\u001b[39m generate_summary([change_points], [scores], [n_frames], [picks])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 51\u001b[0m     f_score \u001b[38;5;241m=\u001b[39m evaluate_summary(summary, user_summary, eval_method)\n\u001b[0;32m     52\u001b[0m     video_fscores\u001b[38;5;241m.\u001b[39mappend(f_score)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Save everything\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eval_method' is not defined"
     ]
    }
   ],
   "source": [
    "# H5 Creation\n",
    "with h5py.File(h5_file_path, 'w') as f:\n",
    "    for filename in os.listdir(video_folder):\n",
    "        if not filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "        \n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sampling_interval = int(round(original_fps / fps))\n",
    "\n",
    "        frames = []\n",
    "        picks = []\n",
    "        count = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if count % sampling_interval == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image = Image.fromarray(frame_rgb)\n",
    "                input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    feature_map = feature_extractor(input_tensor)\n",
    "                    feature = feature_map.squeeze(0)\n",
    "                    frames.append(feature.cpu().numpy())\n",
    "                    picks.append(count)\n",
    "            count += 1\n",
    "\n",
    "        cap.release()\n",
    "        features = np.vstack(frames).astype(np.float32)\n",
    "        picks = np.array(picks)\n",
    "        n_frames = total_frames\n",
    "        n_steps = len(picks)\n",
    "\n",
    "        # Segments\n",
    "        change_points, n_frame_per_seg = get_change_points(picks, n_frames)\n",
    "        \n",
    "        # Generate user summary\n",
    "        user_summary = generate_summary([change_points], [scores], [n_frames], [picks])[0] \n",
    "        \n",
    "        # Scores from model\n",
    "        with torch.no_grad():\n",
    "            scores, _ = model(frames_tensor)  # [1, seq_len]\n",
    "            scores = scores.squeeze(0).cpu().numpy().tolist()\n",
    "            summary = generate_summary([change_points], [scores], [n_frames], [picks])[0]\n",
    "            f_score = evaluate_summary(summary, user_summary, eval_method)\n",
    "            video_fscores.append(f_score)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Save everything\n",
    "        f.create_dataset(video_name + '/features', data=features)\n",
    "        f.create_dataset(video_name + '/gtscore', data=scores)\n",
    "        f.create_dataset(video_name + '/user_summary', data=user_summary)\n",
    "        f.create_dataset(video_name + '/change_points', data=change_points)\n",
    "        f.create_dataset(video_name + '/n_frame_per_seg', data=n_frame_per_seg)\n",
    "        f.create_dataset(video_name + '/n_frames', data=n_frames)\n",
    "        f.create_dataset(video_name + '/picks', data=picks)\n",
    "        f.create_dataset(video_name + '/n_steps', data=n_steps)\n",
    "        f.create_dataset(video_name + '/gtsummary', data=user_summary)\n",
    "        f.create_dataset(video_name + '/video_name', data=np.string_(video_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5781ec37-e512-4755-a908-1c459b34190f",
   "metadata": {},
   "source": [
    "## Perivious Work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43078144-9193-4d39-8365-a74d35769ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GoogLeNet Feature Extractor (before avgpool & FC) ===\n",
    "googlenet = models.googlenet(pretrained=True)\n",
    "feature_extractor = torch.nn.Sequential(*list(googlenet.children())[:-2])\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee954d-bcdd-40b6-bb54-c62ed607f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6ff10-33b1-4e55-b637-d66e88a3cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Video Processing Loop ===\n",
    "for filename in os.listdir(video_folder):\n",
    "    if not filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "        continue\n",
    "\n",
    "    video_path = os.path.join(video_folder, filename)\n",
    "    video_name = os.path.splitext(filename)[0]\n",
    "    output_path = os.path.join(output_folder, f'{dataset_prefix}_{video_name}.h5')\n",
    "\n",
    "    print(f'Processing {video_name}...')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    sampling_interval = int(round(original_fps / fps))\n",
    "\n",
    "    frames = []\n",
    "    picks = []\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % sampling_interval == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = transform(frame_rgb).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                feature_map = feature_extractor(input_tensor)  # shape: [1, C, H, W]\n",
    "                feature = feature_map.mean(dim=[2, 3]).squeeze(0) \n",
    "                frames.append(feature.numpy())\n",
    "                picks.append(count)\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    features = np.vstack(frames).astype(np.float32)\n",
    "    picks = np.array(picks)\n",
    "    n_frames = count\n",
    "    user_summary = np.zeros((1, len(picks)))  # Dummy summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d697c1-6621-4e16-b06f-c7c36dfbad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb70bb-7c1b-46c0-bfae-1080b37da24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the list of 1D arrays into a 2D array: [seq_len, feature_dim]\n",
    "frames_np = np.stack(frames)  # shape: [seq_len, feature_dim]\n",
    "\n",
    "# Convert to tensor and add batch dimension: [1, seq_len, feature_dim]\n",
    "frames_tensor = torch.tensor(frames_np, dtype=torch.float32).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c37f9e-309a-4e79-9ea5-f37a2b92d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PGL_SUM(input_size=1024, output_size=1024, num_segments=4, heads=8,\n",
    "                                fusion=\"add\", pos_enc=\"absolute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2351c5-8026-4385-bcd7-d75dd48614d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "            scores, _ = model(frames_tensor)  # [1, seq_len]\n",
    "            scores = scores.squeeze(0).cpu().numpy().tolist()\n",
    "            summary = generate_summary([sb], [scores], [n_frames], [positions])[0]\n",
    "            f_score = evaluate_summary(summary, user_summary, eval_method)\n",
    "            video_fscores.append(f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc43645-2968-41a8-b2de-0f2893371767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed482b5-1c6b-41b3-87f7-67dc3084b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with h5py.File(output_path, 'w') as hdf:\n",
    "        hdf.create_dataset('features', data=features)\n",
    "        hdf.create_dataset('picks', data=picks)\n",
    "        hdf.create_dataset('n_frame', data=n_frames)\n",
    "        hdf.create_dataset('user_summary', data=user_summary)\n",
    "        hdf.create_dataset('video_name', data=video_name.encode())\n",
    "\n",
    "    print(f'Saved: {output_path}')\n",
    "\n",
    "print(\"âœ… Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
