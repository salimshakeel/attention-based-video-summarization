{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39140f8-3243-4d4c-97c2-221dc4a67ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10692f81-9dda-4912-ac50-4a4bee7af0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_summary import generate_summary\n",
    "from evaluation_metrics import evaluate_summary\n",
    "from layers.summarizer import PGL_SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41cc93ca-00d0-486c-9fad-149906d8dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "video_folder = './Videos/'  # Folder containing your input videos\n",
    "h5_file_path = os.path.join(\"h5file_folder\", \"my_data.h5\")  # Output .h5 files\n",
    "dataset_prefix = 'custom_dataset_01'\n",
    "fps = 15  # Target frame sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67101d5c-28ce-44e4-83f3-6171c175969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(h5_file_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212006ff-0b47-420d-8090-783b086d2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da37971-98a7-4dc3-aaa1-6230883d78e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mobility\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mobility\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load pretrained GoogLeNet\n",
    "googlenet = models.googlenet(pretrained=True)\n",
    "googlenet = googlenet.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c30df8-9aca-4a24-9bf5-0eadb315351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Modify it to return features from 'avgpool' (1024-dim)\n",
    "feature_extractor = torch.nn.Sequential(\n",
    "    googlenet.conv1,\n",
    "    googlenet.maxpool1,\n",
    "    googlenet.conv2,\n",
    "    googlenet.conv3,\n",
    "    googlenet.maxpool2,\n",
    "    googlenet.inception3a,\n",
    "    googlenet.inception3b,\n",
    "    googlenet.maxpool3,\n",
    "    googlenet.inception4a,\n",
    "    googlenet.inception4b,\n",
    "    googlenet.inception4c,\n",
    "    googlenet.inception4d,\n",
    "    googlenet.inception4e,\n",
    "    googlenet.maxpool4,\n",
    "    googlenet.inception5a,\n",
    "    googlenet.inception5b,\n",
    "    googlenet.avgpool,  # shape: [1, 1024, 1, 1]\n",
    "    torch.nn.Flatten(),  # shape: [1, 1024]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf1a046a-7451-4285-b463-b5cf5733deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Image pre-processing transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "        std=[0.229, 0.224, 0.225]    # ImageNet stds\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "088d68a4-06b3-4d29-9574-68f24714acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1a2f770-4513-4ed6-b90c-ab8c278b86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract features from video\n",
    "def extract_video_features(video_path, frame_rate=15):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frames = []\n",
    "    picks = []\n",
    "    count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if int(count % round(fps // frame_rate)) == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame_rgb)\n",
    "            input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feature = feature_extractor(input_tensor)  # shape: [1, 1024]\n",
    "                frames.append(feature.squeeze(0).cpu().numpy())  # shape: [1024]\n",
    "                picks.append(count)\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames, picks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "448e54f3-7cc6-4ee8-a74c-735c7f7dcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"./Videos/1585497-hd_1920_1080_30fps.mp4\"  # replace with your actual video path\n",
    "frames, picks = extract_video_features(video_path)\n",
    "frames_np = np.stack(frames)  # shape: [T, 1024]\n",
    "frames_tensor = torch.tensor(frames_np, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3755fdfc-1b84-4014-8721-ea473a725b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PGL_SUM(input_size=1024, output_size=1024, num_segments=4, heads=8,\n",
    "#                                 fusion=\"add\", pos_enc=\"absolute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb8dd14-c890-401a-a1e3-301dec63b425",
   "metadata": {},
   "source": [
    "### Here i implement the PGL-trained model with best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1233ce5-f555-4bdf-a0c9-5bd12fbf152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from os.path import join\n",
    "\n",
    "# Assuming you already have PGL_SUM class defined/imported\n",
    "trained_model = PGL_SUM(\n",
    "    input_size=1024,\n",
    "    output_size=1024,\n",
    "    num_segments=4,\n",
    "    heads=8,\n",
    "    fusion=\"add\",\n",
    "    pos_enc=\"absolute\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce02aca8-e20c-41f9-a1ba-6be5caff02f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PGL_SUM(\n",
       "  (attention): MultiAttention(\n",
       "    (attention): SelfAttention(\n",
       "      (Wk): ModuleList(\n",
       "        (0-7): 8 x Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (Wq): ModuleList(\n",
       "        (0-7): 8 x Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (Wv): ModuleList(\n",
       "        (0-7): 8 x Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (local_attention): ModuleList(\n",
       "      (0-3): 4 x SelfAttention(\n",
       "        (Wk): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=1024, out_features=64, bias=False)\n",
       "        )\n",
       "        (Wq): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=1024, out_features=64, bias=False)\n",
       "        )\n",
       "        (Wv): ModuleList(\n",
       "          (0-3): 4 x Linear(in_features=1024, out_features=64, bias=False)\n",
       "        )\n",
       "        (out): Linear(in_features=256, out_features=1024, bias=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (drop): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear_1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (linear_2): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (norm_y): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (norm_linear): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model weights\n",
    "model_path = \"./Model/epoch-199.pkl\"       # e.g., './models'\n",
    "trained_model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ac27a89-ae5d-4ec6-a5a6-ed224a76145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video score: tensor([[0.2575, 0.2028, 0.3044,  ..., 0.1520, 0.1577, 0.1427]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    scores, _ = trained_model(frames_tensor)\n",
    "    print(\"Video score:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7b8c1-c951-4268-b96c-f6282366094d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23d932c-bf5c-4c70-93d7-1d82b8ac554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_change_points(picks, n_frames, segments=5):\n",
    "    seg_len = len(picks) // segments\n",
    "    change_points = []\n",
    "    frame_per_seg = []\n",
    "    for i in range(segments):\n",
    "        start = i * seg_len\n",
    "        end = (i + 1) * seg_len - 1 if i < segments - 1 else len(picks) - 1\n",
    "        change_points.append([picks[start], picks[end]])\n",
    "        frame_per_seg.append(picks[end] - picks[start] + 1)\n",
    "    return np.array(change_points), np.array(frame_per_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c838f-cdc8-4b80-956e-359bdfb91e35",
   "metadata": {},
   "source": [
    "### This is the score i get earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e63076-104a-4ad0-aec4-850de731a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scores, _ = model(frames_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3fe6eb-2f59-43ed-977b-29ea0278ef5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3929, 0.3536, 0.7351,  ..., 0.4774, 0.4257, 0.3886]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979e430f-b882-4343-8677-77bad72fa73a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1260,) into shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m change_points, n_frame_per_seg \u001b[38;5;241m=\u001b[39m get_change_points(picks, n_frames)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Generate user summary\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m user_summary \u001b[38;5;241m=\u001b[39m generate_summary([change_points], [scores], [n_frames], [picks])[\u001b[38;5;241m0\u001b[39m] \n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Scores from model\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\Video_Summerization_Pipline\\attention-based-video-summarization\\generate_summary.py:35\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[1;34m(all_shot_bound, all_scores, all_nframes, all_positions)\u001b[0m\n\u001b[0;32m     33\u001b[0m         frame_scores[pos_left:pos_right] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m         frame_scores[pos_left:pos_right] \u001b[38;5;241m=\u001b[39m frame_init_scores[i]\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Compute shot-level importance scores by taking the average importance scores of all frames in the shot\u001b[39;00m\n\u001b[0;32m     38\u001b[0m shot_imp_scores \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (1260,) into shape (2,)"
     ]
    }
   ],
   "source": [
    "# H5 Creation\n",
    "with h5py.File(h5_file_path, 'w') as f:\n",
    "    for filename in os.listdir(video_folder):\n",
    "        if not filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_folder, filename)\n",
    "        \n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sampling_interval = int(round(original_fps / fps))\n",
    "\n",
    "        frames = []\n",
    "        picks = []\n",
    "        count = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if count % sampling_interval == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image = Image.fromarray(frame_rgb)\n",
    "                input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    feature_map = feature_extractor(input_tensor)\n",
    "                    feature = feature_map.squeeze(0)\n",
    "                    frames.append(feature.cpu().numpy())\n",
    "                    picks.append(count)\n",
    "            count += 1\n",
    "\n",
    "        cap.release()\n",
    "        features = np.vstack(frames).astype(np.float32)\n",
    "        picks = np.array(picks)\n",
    "        n_frames = total_frames\n",
    "        n_steps = len(picks)\n",
    "\n",
    "        # Segments\n",
    "        change_points, n_frame_per_seg = get_change_points(picks, n_frames)\n",
    "        \n",
    "        # Generate user summary\n",
    "        user_summary = generate_summary([change_points], [scores], [n_frames], [picks])[0] \n",
    "        \n",
    "        # Scores from model\n",
    "        with torch.no_grad():\n",
    "            scores, _ = model(frames_tensor)  # [1, seq_len]\n",
    "            #scores = scores.squeeze(0).cpu().numpy().tolist()\n",
    "            #summary = generate_summary([change_points], [scores], [n_frames], [picks])[0]\n",
    "            #f_score = evaluate_summary(summary, user_summary, eval_method)\n",
    "            #video_fscores.append(f_score)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d347e5bb-a025-4e11-ac10-1acb5c78fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "        # Save everything\n",
    "        f.create_dataset(video_name + '/features', data=features)\n",
    "        f.create_dataset(video_name + '/gtscore', data=scores)\n",
    "        f.create_dataset(video_name + '/user_summary', data=user_summary)\n",
    "        f.create_dataset(video_name + '/change_points', data=change_points)\n",
    "        f.create_dataset(video_name + '/n_frame_per_seg', data=n_frame_per_seg)\n",
    "        f.create_dataset(video_name + '/n_frames', data=n_frames)\n",
    "        f.create_dataset(video_name + '/picks', data=picks)\n",
    "        f.create_dataset(video_name + '/n_steps', data=n_steps)\n",
    "        f.create_dataset(video_name + '/gtsummary', data=user_summary)\n",
    "        f.create_dataset(video_name + '/video_name', data=np.string_(video_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5781ec37-e512-4755-a908-1c459b34190f",
   "metadata": {},
   "source": [
    "## Perivious Work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43078144-9193-4d39-8365-a74d35769ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === GoogLeNet Feature Extractor (before avgpool & FC) ===\n",
    "# googlenet = models.googlenet(pretrained=True)\n",
    "# feature_extractor = torch.nn.Sequential(*list(googlenet.children())[:-2])\n",
    "# feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee954d-bcdd-40b6-bb54-c62ed607f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6ff10-33b1-4e55-b637-d66e88a3cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Video Processing Loop ===\n",
    "# for filename in os.listdir(video_folder):\n",
    "#     if not filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "#         continue\n",
    "\n",
    "#     video_path = os.path.join(video_folder, filename)\n",
    "#     video_name = os.path.splitext(filename)[0]\n",
    "#     output_path = os.path.join(output_folder, f'{dataset_prefix}_{video_name}.h5')\n",
    "\n",
    "#     print(f'Processing {video_name}...')\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     sampling_interval = int(round(original_fps / fps))\n",
    "\n",
    "#     frames = []\n",
    "#     picks = []\n",
    "#     count = 0\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         if count % sampling_interval == 0:\n",
    "#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             input_tensor = transform(frame_rgb).unsqueeze(0)\n",
    "#             with torch.no_grad():\n",
    "#                 feature_map = feature_extractor(input_tensor)  # shape: [1, C, H, W]\n",
    "#                 feature = feature_map.mean(dim=[2, 3]).squeeze(0) \n",
    "#                 frames.append(feature.numpy())\n",
    "#                 picks.append(count)\n",
    "#         count += 1\n",
    "\n",
    "#     cap.release()\n",
    "#     features = np.vstack(frames).astype(np.float32)\n",
    "#     picks = np.array(picks)\n",
    "#     n_frames = count\n",
    "#     user_summary = np.zeros((1, len(picks)))  # Dummy summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d697c1-6621-4e16-b06f-c7c36dfbad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb70bb-7c1b-46c0-bfae-1080b37da24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stack the list of 1D arrays into a 2D array: [seq_len, feature_dim]\n",
    "# frames_np = np.stack(frames)  # shape: [seq_len, feature_dim]\n",
    "\n",
    "# # Convert to tensor and add batch dimension: [1, seq_len, feature_dim]\n",
    "# frames_tensor = torch.tensor(frames_np, dtype=torch.float32).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c37f9e-309a-4e79-9ea5-f37a2b92d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PGL_SUM(input_size=1024, output_size=1024, num_segments=4, heads=8,\n",
    "#                                 fusion=\"add\", pos_enc=\"absolute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2351c5-8026-4385-bcd7-d75dd48614d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#             scores, _ = model(frames_tensor)  # [1, seq_len]\n",
    "#             scores = scores.squeeze(0).cpu().numpy().tolist()\n",
    "#             summary = generate_summary([sb], [scores], [n_frames], [positions])[0]\n",
    "#             f_score = evaluate_summary(summary, user_summary, eval_method)\n",
    "#             video_fscores.append(f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc43645-2968-41a8-b2de-0f2893371767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed482b5-1c6b-41b3-87f7-67dc3084b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     with h5py.File(output_path, 'w') as hdf:\n",
    "#         hdf.create_dataset('features', data=features)\n",
    "#         hdf.create_dataset('picks', data=picks)\n",
    "#         hdf.create_dataset('n_frame', data=n_frames)\n",
    "#         hdf.create_dataset('user_summary', data=user_summary)\n",
    "#         hdf.create_dataset('video_name', data=video_name.encode())\n",
    "\n",
    "#     print(f'Saved: {output_path}')\n",
    "\n",
    "# print(\"âœ… Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
